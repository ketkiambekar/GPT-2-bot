{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_generator.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ketkiambekar/ResiBot/blob/master/text_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jap7zePIp7aB",
        "outputId": "d0476484-77c0-46ad-da9f-e5cb834153ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# This cell mounts the Google Drive. Upload the text file containing the training text, to your google drive. \n",
        "# This code assumes that the file is on the root of your google drive, but if you have it in a sub folder, then you need to modify the path in line 7.\n",
        "# Line 9 displays all the file present in the folder path provided in line 7\n",
        "# Run the cell, click on the link that appears, login and enter the code in the box to mount the drive. \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/My Drive/AIR\n",
        "%ls\n",
        "%pwd\n",
        "#%reset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/My Drive/AIR\n",
            "\u001b[0m\u001b[01;34mcheckpoint\u001b[0m/  f_corpus.txt  \u001b[01;34mmodels\u001b[0m/  Nogo.txt  \u001b[01;34msamples\u001b[0m/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/AIR'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8kNE_fmqCAJ",
        "outputId": "5c22b3fa-e772-430c-9e70-be2c8a15e1d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esXqAoBiYGIb"
      },
      "source": [
        "We are able to use the same function for generating text with or without prompts (by passing a blank string to the text_gen function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEbaWX8Xo3ho",
        "outputId": "3a2b43f6-01d2-4c61-9bcd-7973c7de6e72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import tensorflow as tf\n",
        "\n",
        "import sys\n",
        "from io import StringIO \n",
        "\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-151baa01d371>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgpt_2_simple\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgpt_2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgpt_2_simple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_saving_gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgpt_2_simple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgpt_2_simple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccumulatingOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/memory_saving_gradients.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_editor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3KiLnxf0DVo"
      },
      "source": [
        "def gpt2_gen(prefix1, temperature1, length1,run_name1):\n",
        "\n",
        "  tf.reset_default_graph()\n",
        "  sess = gpt2.start_tf_sess()\n",
        "\n",
        "  gpt2.load_gpt2(sess, run_name=run_name1)\n",
        "\n",
        "  #capture generated text into a buffer instead of printing it on console\n",
        "  old_stdout = sys.stdout\n",
        "  sys.stdout = mystdout = StringIO()\n",
        "\n",
        "  gpt2.generate(sess,model_name='124M',length=length1,temperature=temperature1,prefix=prefix1,nsamples=1,batch_size=1)\n",
        "  sys.stdout = old_stdout\n",
        "\n",
        "  gen = mystdout.getvalue()\n",
        "      \n",
        "  return gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0oOfnKqyuDe"
      },
      "source": [
        "def text_gen(prompt, temperature, length,run_name1, attempts):\n",
        "\n",
        "  prompt=\" \"+prompt+\" \" \n",
        "  for i in range (0,attempts):\n",
        "\n",
        "    check=0\n",
        "\n",
        "    #Invoke generate function of GPT-2 model\n",
        "    gen = gpt2_gen(prompt,temperature,length,run_name1)\n",
        "\n",
        "    #If there are quotation marks in generated text, re-generate\n",
        "    if not \"\\\"\" in gen :\n",
        "      check+=1\n",
        "   \n",
        "    #If there is year in the text (4 digit number), re-generate\n",
        "    if len(re.findall(\" \\d{4} \", \" \"+gen+\" \"))==0:\n",
        "      check+=1\n",
        "\n",
        "    if check==2:\n",
        "      break\n",
        "\n",
        "  #Remove Extra White Spaces\n",
        "  gen = ' '.join(gen.split())\n",
        " \n",
        "  #Replace Covid 19 by covid 32\n",
        "  gen= gen.replace('COVID-19', 'COVID-32').replace('covid-19', 'covid-32').replace('Covid-19','Covid-32').replace('COVID19', 'COVID32').replace('Covid19','Covid32').replace('covid19','covid32')\n",
        "  \n",
        "  #Remove Quotes (and some special characters that look like quotes but are not)\n",
        "  gen=gen.replace('\\\"', '').replace(\"”\",\"\").replace(\"“\",\"\")\n",
        "\n",
        "  return gen\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPGcEsiVG75M"
      },
      "source": [
        "# import re\n",
        "# stri = \"She said \\\" Hello in 1933\"\n",
        "# if  re.findall(\" \\d{4} \", \" \"+stri+\" \"):\n",
        "#   print(\"yes\")\n",
        "# else:\n",
        "#   print(\"no\")\n",
        "\n",
        "# stri = stri.replace('\\\"', '')\n",
        "# print(stri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfYeV7-1rjyN"
      },
      "source": [
        "prompt=\"techno-utopians\"  #----------------> Set any text\n",
        "temperature = 0.7 #---------------->Set a number between 0 and 1\n",
        "length = 100      #-----------------> Set desired number of words in the text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3UunI2zp6z9",
        "outputId": "815c504f-bb36-4fa9-b93e-9e3513b27258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "gen = text_gen(prompt,temperature,length,'AIR',5)\n",
        "\n",
        "print(\"Output:\" + gen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/AIR/model-1000\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/AIR/model-1000\n",
            "Output:techno-utopians have a problem with the power and dollars they expend to keep business as usual. As a result, some drug lords and the organized crime groups that use them have been colluding with each other to try to control the markets and ensure that the market will not be allowed to be rigged. The governments that do this are working to keep the drugs flowing to the criminals and drug trafficking networks. For example, the United States has been supporting drug traffickers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdYbZ4saLRN3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}